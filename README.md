                                                        Transformer Model from Scratch using PyTorch
This repository contains a complete implementation of the Transformer architecture (originally introduced in "Attention is All You Need") using PyTorch, built entirely from scratch.

                                      Features
Layer Normalization

Multi-Head Attention Mechanism

Positional Encoding

Feedforward Layers

Residual Connections with Dropout

Stacked Encoder and Decoder Blocks

Final Projection Layer
